{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:37: UserWarning: You are currently using a nightly version of TensorFlow (2.9.0-dev20220127). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as hub_text\n",
    "from official import nlp\n",
    "import official.nlp.optimization\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import math\n",
    "import official.nlp.bert.tokenization\n",
    "from official.nlp import bert\n",
    "import string\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "\n",
    "def encode_sentence(s, tokenizer):\n",
    "    '''\n",
    "    Tokenizes pair of sentences and adds a [SEP] token to join them. This token is labeled as 0\n",
    "    '''\n",
    "    tokens = list(tokenizer.tokenize(s))\n",
    "    tokens.append('[SEP]')\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "\n",
    "\n",
    "def bert_encode(sentence_dict, tokenizer):\n",
    "    '''\n",
    "    Preprocess the data to be on the format expected by BERT. Does the same\n",
    "    as the BERT preprocessor function.\n",
    "    Input:\n",
    "        1. Dict containing:\n",
    "            'sentence1':\n",
    "                ['These results indicate that the GeneReg'], \n",
    "            'sentence2':\n",
    "                ['and acrD drug efflux genes are directly regulated by RegProtein protein ( BaeR protein ) .']\n",
    "        2. Labels:\n",
    "            [''O O O O O O 0 I-Rel I-Rel I-Rel I-Rel I-Rel I-Rel I-Rel I-Rel I-Rel O O O O O O O O]\n",
    "    '''\n",
    "    num_examples = len(sentence_dict[\"gene1\"])\n",
    "\n",
    "    sentence1 = tf.ragged.constant([\n",
    "        encode_sentence(s, tokenizer)\n",
    "        for s in np.array(sentence_dict[\"gene1\"])])\n",
    "    sentence2 = tf.ragged.constant([\n",
    "        encode_sentence(s, tokenizer)\n",
    "        for s in np.array(sentence_dict[\"gene2\"])])\n",
    "\n",
    "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n",
    "    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
    "\n",
    "    input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "\n",
    "    type_cls = tf.zeros_like(cls)\n",
    "    type_s1 = tf.zeros_like(sentence1)\n",
    "    type_s2 = tf.ones_like(sentence2)\n",
    "    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
    "\n",
    "    inputs = {\n",
    "        'input_word_ids': input_word_ids.to_tensor(),\n",
    "        'input_mask': input_mask,\n",
    "        'input_type_ids': input_type_ids}\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\n",
    "def sentence_token_tagging(test_sentence_tags, tokenized_sentences):\n",
    "    '''\n",
    "    Rewrite the genes found by the NER model. \n",
    "    Input: \n",
    "        1. List of categorical labels asigned by the model during prediction. \n",
    "        2. List of tokenized sentences. (BERT tokenizer)\n",
    "        \n",
    "    Output:\n",
    "        1. NerSentence: Sentences in which each found entity was replaced by the word GENE\n",
    "            [The GENE protein has two activation domains , one of which is an GENE ...]\n",
    "            \n",
    "        2. FinalEntities: All of the entities in the sentence that were replaced by the word GENE\n",
    "            [AraC, arac xyls family domain...]\n",
    "        \n",
    "    '''\n",
    "    entity = ''\n",
    "    num_entities = 0\n",
    "    n = 0\n",
    "    TF_Regulator, RegulatedGene = [],[]\n",
    "    FinalEntities, temp,temp_s,NerSentence = [],[],[],[]\n",
    "    for num in (range(len(test_sentence_tags))): \n",
    "        for num_word, (entity_tags, words) in enumerate(zip(test_sentence_tags[num], tokenized_sentences[num])):\n",
    "            if entity_tags.startswith('B'):\n",
    "                entity += '[SEP] ' + str(words) + ' '\n",
    "                num_entities += 1\n",
    "                temp_s.append('GENE')\n",
    "                \n",
    "            if entity_tags.startswith('I'):\n",
    "                if test_sentence_tags[num][num_word-1].startswith('O'):\n",
    "                    entity += '[SEP] ' + str(words) + ' '\n",
    "                    num_entities += 1\n",
    "                    temp_s.append('GENE')\n",
    "                       \n",
    "                else:\n",
    "                    entity += str(words) + ' '\n",
    "            \n",
    "            if entity_tags.startswith('B') == False and entity_tags.startswith('I') == False:\n",
    "                temp_s.append(words)\n",
    "    \n",
    "        if entity != '':\n",
    "            temp.append(entity.split('[SEP] ')[1:])        \n",
    "        FinalEntities.append(temp)\n",
    "        entity = ''\n",
    "        temp_str = \" \".join(temp_s).replace('  ',' ')\n",
    "        NerSentence.append(temp_str.split(' '))\n",
    "        temp_s = []\n",
    "        temp = []\n",
    "\n",
    "\n",
    "        \n",
    "    print(f'Completed. Found {num_entities} genes.')\n",
    "    return [NerSentence,FinalEntities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 176s 5s/step\n",
      "Completed. Found 4533 genes.\n"
     ]
    }
   ],
   "source": [
    "# Loads the file with the example data and tensorflowhub preprocessor\n",
    "tokenizer = bert.tokenization.FullTokenizer('vocabNER.txt', do_lower_case=False)\n",
    "preprocessor = hub.load( \"https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\")\n",
    "with open('../Tools/ris-sentences-ECO.txt','r') as file:\n",
    "    eco_sentence = file.readlines()\n",
    "\n",
    "\n",
    "raw_text = preprocessor(eco_sentence) # Preprocessing sentences\n",
    "bert_classifier = tf.keras.models.load_model('NERModel')  # Loads the NER model trained with the Ner_Training.ipynb notebook\n",
    "prediction = bert_classifier.predict(raw_text) # Predicts the label for each token of the preprocessed sentences\n",
    "\n",
    "# Tokenizes the sentences and removes tokens that are not a whole word \n",
    "pre_txt = []\n",
    "for indx in range(len(eco_sentence)):\n",
    "    pre_txt.append(' '.join(tokenizer.tokenize(eco_sentence[indx])).replace(' ##','').replace('##','').replace('  ',' ').split(' '))\n",
    "    \n",
    "\n",
    "# The \"prediction\" variable has a score for each of the possible categories for a token'\n",
    "#    ([CLS],[SEP],[PAD])        O          B-GENE      I-GENE    -> Index (0:3)\n",
    "#           0.01              0.90         0.05      0.04        -> Predicted score for a single token\n",
    "# THE SIZE OF PREDICTION IS:     (4) x (Num of tokens in a sentence) x (Total num of sentences)\n",
    "# This part finds in which index of \"prediction[sentence x][token x]\" has the bigest number, then saves the asociated label for that index\n",
    "sentence_tags = []\n",
    "raw_sentences = []\n",
    "TF_Regulator_, RegulatedGene_ = [],[]\n",
    "for i, sentence in enumerate(prediction):\n",
    "    temp_rel = []\n",
    "    for n_wor, pred_word in enumerate(sentence):\n",
    "        val = list(pred_word)\n",
    "        if val[1] == max(val):\n",
    "            temp_rel.append('O')\n",
    "        elif val[2] == max(val):\n",
    "            temp_rel.append('B-GENE')\n",
    "        elif val[3] == max(val):\n",
    "            temp_rel.append('I-GENE')\n",
    "    if len(pre_txt[i]) == len(temp_rel):\n",
    "        raw_sentences.append(pre_txt[i])\n",
    "        sentence_tags.append(temp_rel)\n",
    "\n",
    "\n",
    "# Calls the function that hides the found entities.\n",
    "ner_sentence, final_entities  = sentence_token_tagging(sentence_tags,raw_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This part selects all posible pair of entities in a sentence. The 2 selected entities are between a pair of new special tokens [E1]-[/E1] or [E2]-[/E2]\n",
    "# This was aproach is explained here https://medium.com/e-bot7-tech/matching-the-blanks-78e8063794b5 but its not implemented in this code cus it was 2 difficult\n",
    "output_sentence = []\n",
    "for line in range(len(ner_sentence)):\n",
    "    new_line = ' '.join(ner_sentence[line])\n",
    "    genes_in_sentence = 0\n",
    "    prducts_in_sentence = 0\n",
    "    for x in range(len(final_entities[line][0])):\n",
    "        if genes_in_sentence == 0:\n",
    "            new_line = new_line.replace('GENE',f'[ {final_entities[line][0][x]} ]',1).replace('  ',' ').replace(' [ ','[E1]').replace(' ] ','[/E1]')\n",
    "            genes_in_sentence += 1            \n",
    "        elif prducts_in_sentence == 0:\n",
    "            new_line = new_line.replace('GENE',f'[ {final_entities[line][0][x]} ]',1).replace('  ',' ').replace(' [ ','[E2]').replace(' ] ','[/E2]')\n",
    "            prducts_in_sentence += 1\n",
    "                         \n",
    "        else:\n",
    "            new_line = new_line.replace('GENE',f' [{final_entities[line][0][x]}] ',1).replace('  ',' ')\n",
    "                \n",
    "    if '[/E2]' in new_line and '[/E1]' in new_line:\n",
    "        output_sentence.append(new_line.replace('  ',' '))\n",
    "\n",
    "\n",
    "\n",
    "# By this part we have sentences in wich two pairs of entities are delimited by [E] and [/E]\n",
    "# Example:\n",
    "##  [E1]AraC[/E1] seems to slightly repress [E2]arac[/E2] ( i . e . , below our cut - off level of 2 . 5 - fold ) .\n",
    "input_sentences = []\n",
    "for e,x in enumerate(output_sentence):\n",
    "    if x[0] == ' ':\n",
    "        output_sentence[e] = output_sentence[e][1:]\n",
    "    temp = ''\n",
    "    list_rel = []\n",
    "    n = 0\n",
    "    for i, word in enumerate(x.split(' ')):\n",
    "        temp += word + ' '\n",
    "        if n == 2:\n",
    "            list_rel[-1] += word + ' '\n",
    "            \n",
    "        if '[/E1]' in word : # All the words located before the end of the first entity ([/E1]) make sentence 1:     i.e [AraC]\n",
    "            list_rel.append(temp)\n",
    "            if n < 1:\n",
    "                temp = ''\n",
    "            n += 1\n",
    "            \n",
    "        if '[/E2]' in word: # All the words located after the end of the first entity ([/E1]) make sentence 2:      i.e [seems to slightly repress arac ( i . e . , below our cut - off level of 2 . 5 - fold ) . ]\n",
    "            list_rel.append(temp)\n",
    "            if n < 1:\n",
    "                temp = ''\n",
    "            n += 1\n",
    "    # The result is the concatenation of sentence 1 and sentence 2 but with a [SEP] token between em.\n",
    "    # [[AraC [SEP] seems to slightly repress arac ( i . e . , below our cut - off level of 2 . 5 - fold ) . ]]\n",
    "    input_sentences.append('[SEP] '.join(list_rel).replace('[/E1]','] ').replace('[/E2]','] ').replace('[E1]',' [').replace('[E2]',' [').replace('  ',' '))\n",
    "    \n",
    "\n",
    "labels = []\n",
    "sentences = []\n",
    "for indx in range(len(input_sentences)):\n",
    "    temp_label = []\n",
    "    n = 0\n",
    "    for word in input_sentences[indx].split(' '):\n",
    "        if n == 1:\n",
    "            temp_label.append('I-Rel') # Only the words between the pair of entities are labeled as I-Rel. The rest of the words are labeled as O\n",
    "        if '[SEP]' in word:\n",
    "            n = 1\n",
    "        if word == 'GeneReg' or word == 'RegProtein':\n",
    "            n = 0\n",
    "            temp_label.append('O') \n",
    "        else:\n",
    "            if n == 0:\n",
    "                temp_label.append('O')\n",
    "    # Split each  sentence into sentence 1 and sentence 2 (as in previous chunk)\n",
    "    sentences.append(input_sentences[indx].replace('[','').replace(']','').replace('  ',' ').split('SEP '))\n",
    "    labels.append(temp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 218s 7s/step\n",
      "The O\n",
      "results O\n",
      "of O\n",
      "the O\n",
      "present O\n",
      "study O\n",
      "extend O\n",
      "these O\n",
      "observations O\n",
      "and O\n",
      "show O\n",
      "that O\n",
      "the O\n",
      "Met O\n",
      "R PAD\n",
      "protein O\n",
      "also O\n",
      "stimulate PAD\n",
      "s PAD\n",
      "the I-Rel\n",
      "in O\n",
      "v O\n",
      "it I-Rel\n",
      "ro I-Rel\n",
      "expression O\n",
      "of I-Rel\n",
      "met I-Rel\n",
      "H I-Rel\n",
      "and I-Rel\n",
      "that I-Rel\n",
      "both I-Rel\n",
      "the PAD\n",
      "Met I-Rel\n",
      "E I-Rel\n",
      "and I-Rel\n",
      "Met I-Rel\n",
      "H I-Rel\n",
      "proteins O\n",
      "synthesized PAD\n",
      "in PAD\n",
      "v PAD\n",
      "it O\n",
      "ro O\n",
      "are O\n",
      "en PAD\n",
      "zy O\n",
      "matical O\n",
      "ly I-Rel\n",
      "active O\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_classifier = tf.keras.models.load_model('REModel/')\n",
    "tokenizer = bert.tokenization.FullTokenizer(vocab_file=\"vocabNER.txt\",do_lower_case=False)\n",
    "\n",
    "\n",
    "sentence_test = {}\n",
    "gene1_test = []\n",
    "gene2_test = []\n",
    "full_sentence = []\n",
    "for x in sentences:    \n",
    "    gene1_test.append(x[0])\n",
    "    gene2_test.append(x[0] + x[1])\n",
    "    full_sentence.append(x[0] + x[1])\n",
    "sentence_test['gene1'] = gene1_test\n",
    "sentence_test['gene2'] = gene2_test\n",
    "\n",
    "\n",
    "pre_txt = []\n",
    "for indx in range(len(full_sentence)):\n",
    "    pre_txt.append(' '.join(tokenizer.tokenize(full_sentence[indx])).replace('  ',' ').split(' '))\n",
    "    \n",
    "test_text = bert_encode(sentence_test, tokenizer)\n",
    "prediction = bert_classifier.predict(test_text)\n",
    "# for x in pre_txt:\n",
    "#     x.extend(['PAD']*(128-len(x)))\n",
    "\n",
    "\n",
    "example_tags = []\n",
    "raw_example = []\n",
    "for i, sentence in enumerate(prediction):\n",
    "    temp_rel = []\n",
    "    for n_wor, pred_word in enumerate(sentence[:len(pre_txt[i])]):\n",
    "        val = list(pred_word)\n",
    "        if val[1] == max(val):\n",
    "            temp_rel.append('O')\n",
    "        if val[2] == max(val):\n",
    "            temp_rel.append('I-Rel')\n",
    "        if val[0] == max(val):\n",
    "            temp_rel.append('PAD')\n",
    "    if len(pre_txt[i]) == len(temp_rel):\n",
    "        raw_example.append(pre_txt[i])\n",
    "        example_tags.append(temp_rel)\n",
    "\n",
    "\n",
    "for i,x in enumerate(raw_example[0]):\n",
    "        print(x.replace(' ##','').replace('##',''), example_tags[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of the present study extend these observations and show that the MetR protein also stimulates the in **vitro expression of metH and that both the MetE and MetH proteins synthesized in vitro are enzymatically active .** \n",
      "Operons observed to be differentially expressed include not only all four of the transcripts , hmpA , ytfE , ygbA , and hcp - hcr , known or predicted to be **NsrR - regulated ( 4 , 40 ) , but also other transcripts predicted to be regulated by nitrite or RNS generated from nitrite , for example , nitric oxide ( 11 ) .** \n",
      "It appears that binding of the metJ gene product may prevent binding **of RNA polymerase to the metB and Jl promoters and , under strongly repressing conditions , may also decrease the binding to the J2 promoter .** \n",
      "Both inaA and marRAB are known Rob - regulated genes ( 5 , 21 **) .** \n",
      "Identification of the DNAbinding domain of the OmpR protein required for transcriptional activation of the ompF and ompC **genes of Escherichia coli by in vivo DNA footprinting .** \n",
      "MelR is a member of **the AraC - XylS family of bacterial gene regulatory proteins ( 6 ) and our previous studies have shown that MelR , together with the cyclic AMP receptor protein , CRP ,** regulates expression of the melAB operon that encodes products essential for melibiose metabolism ( 7 ) .\n",
      "For example , the nitrate reductase genes narGHJI are regulated **exclusively by NarL and are not responsive to NarP as observed here for cydD , while nitrate induction of the fdnGHI operon is regulated predominantly by NarL but is also responsive in part to NarP ( 33 , 39 ) .** \n",
      "Under identical experimental conditions as for argO , the regulatory regions **of the other ArgP - regulated genes ( asd , dapB , dapD , gdhA , lysA , lysC , and lysP ) were also bound by ArgP , with apparent K d s ranging from 55 nM to 170 nM ; in all these cases ( unlike the situation with argO ) , the addition of Lys was associated with an increase in the apparent K d , indicating that ArgP binding in these instances is Lys sensitive .** \n",
      "MelR - dependent repression of **the melR promoter in each of the new constructs was measured as above and the results are illustrated in** Figure 6B .\n",
      "Given that the activation by full - length , chromosomally expressed RhaS at rhaBAD is approximately 33 - fold higher than that of **chromosomally expressed RhaR at rhaSR , comparable efficiencies of activation by the CTDs to their full - length counterparts would have resulted in His 6 - RhaR - CTD activating rhaSR by approximately 30 - fold .** \n",
      "This observation not only allows us to understand how a modest mutation in O NC2 can affect fimB expression **whereas the D3 mutation does not , but it also supports our prior assertion that NanR activates fimB expression without NagC ( Sohanpal et al . , 2004 ) .** \n",
      "These latter results , together with the experiments described here showing that MetJ binds to this **region of DNA , strongly suggest that the - 8 to + 27 region is involved in metE repression .** \n",
      "Note that in a control experiment , the activity of the MBP - NarL protein was confirmed by **its ability to activate transcription of the NarL - dependent fdnG promoter in vitro ( data not shown ) .** \n",
      "Thus , it appears that NarL and NarP adopt overlapping mechanisms to inhibit ydhY – T expression . **** \n",
      "In relation to pacsP1 , the DNA site for Fis is **centred at position – 61 , and its distal nature suggests that pacsP1 repression operates via a different mechanism .** \n",
      "Our results indicate that yggA encodes an ArgP - regulated Arg exporter **in E . coli .** \n",
      "Organization and regulation of the D - xylose operons in Escherichia coli K - 12 : **XylR acts as a transcriptional activator .** \n",
      "Some of these operons are regulated by the NarL protein alone , such as the narG and **frdA operons , whereas expression of the nirB ( encoding NADH - dependent nitrite reductase ) , nrfA and fdnG operons is controlled by both the NarL and NarP proteins .** \n",
      "The ArgP protein enhances the expression of the argK gene To **test the biological activity of the ArgP protein on the arginine transport system , the amounts of mRNA synthesized by a 502 bp KpnI - EcoRV DNA fragment containing the** N terminus and the control region of the argK gene was investigated .\n",
      "However , the inactivation of DcuS ( IMW553 ) led to a decrease of citC - lacZ expression by **a factor of 1 . 5 , which could be due to interaction between CitA and DcuS .** \n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence_test = {}\n",
    "gene1_test = []\n",
    "gene2_test = []\n",
    "for x in sentences:    \n",
    "    gene1_test.append(x[0])\n",
    "    gene2_test.append(x[1])\n",
    "sentence_test['gene1'] = gene1_test\n",
    "sentence_test['gene2'] = gene2_test\n",
    "\n",
    "\n",
    "sentence_tags = []\n",
    "for sentence in prediction:\n",
    "    temp_rel = []\n",
    "    for n_wor, pred_word in enumerate(sentence):\n",
    "        val = list(pred_word)\n",
    "        if val[2] == max(val):\n",
    "            temp_rel.append(n_wor)\n",
    "    sentence_tags.append(temp_rel)\n",
    "\n",
    "\n",
    "complete_sentence = []\n",
    "for x in range(len(sentence_tags)):\n",
    "    complete_sentence.append(' '.join(tokenizer.tokenize(sentence_test['gene1'][x]) + tokenizer.tokenize(sentence_test['gene2'][x])).replace(' ##','').replace('##','').split(' '))\n",
    "\n",
    "\n",
    "predicted_relations = []\n",
    "for sent_num, word_index in enumerate(sentence_tags):\n",
    "    if len(word_index) > 1:\n",
    "        relationship_end = max(word_index)\n",
    "        relationship_start = min(word_index)\n",
    "        sentence_relation = ' '.join(complete_sentence[sent_num][relationship_start:relationship_end])\n",
    "        sentence_start = ' '.join(complete_sentence[sent_num][:relationship_start]).replace(' ##','').replace('##','')\n",
    "        sentence_end = ' '.join(complete_sentence[sent_num][relationship_end:]).replace(' ##','').replace('##','')\n",
    "    predicted_relations.append(sentence_start + ' **' + sentence_relation + '** ' + sentence_end)\n",
    "    #predicted_relations.append(sentence_relation)\n",
    "\n",
    "for x in predicted_relations[0:20]:\n",
    "    print(x.replace('[ ','[').replace(' ]',']'))\n",
    "    #print(x.replace('[ ','').replace(' ]',''))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee7d7838ef53998fd22ad7449b76e48b4013ea11e59d28ee193f2cd757746339"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
