{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as hub_text\n",
    "from official import nlp\n",
    "import official.nlp.optimization\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import math\n",
    "import official.nlp.bert.tokenization\n",
    "from official.nlp import bert\n",
    "import string\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "\n",
    "def encode_sentence(s, tokenizer):\n",
    "    '''\n",
    "    Tokenizes pair of sentences and adds a [SEP] token to join them. This token is labeled as 0\n",
    "    '''\n",
    "    tokens = list(tokenizer.tokenize(s))\n",
    "    tokens.append('[SEP]')\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "\n",
    "\n",
    "def bert_encode(sentence_dict, tokenizer):\n",
    "    '''\n",
    "    Preprocess the data to be on the format expected by BERT. Does the same\n",
    "    as the BERT preprocessor function.\n",
    "    Input:\n",
    "        1. Dict containing:\n",
    "            'sentence1':\n",
    "                ['These results indicate that the GeneReg'], \n",
    "            'sentence2':\n",
    "                ['and acrD drug efflux genes are directly regulated by RegProtein protein ( BaeR protein ) .']\n",
    "        2. Labels:\n",
    "            [''O O O O O O 0 I-Rel I-Rel I-Rel I-Rel I-Rel I-Rel I-Rel I-Rel I-Rel O O O O O O O O]\n",
    "    '''\n",
    "    num_examples = len(sentence_dict[\"gene1\"])\n",
    "\n",
    "    sentence1 = tf.ragged.constant([\n",
    "        encode_sentence(s, tokenizer)\n",
    "        for s in np.array(sentence_dict[\"gene1\"])])\n",
    "    sentence2 = tf.ragged.constant([\n",
    "        encode_sentence(s, tokenizer)\n",
    "        for s in np.array(sentence_dict[\"gene2\"])])\n",
    "\n",
    "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n",
    "    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
    "\n",
    "    input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "\n",
    "    type_cls = tf.zeros_like(cls)\n",
    "    type_s1 = tf.zeros_like(sentence1)\n",
    "    type_s2 = tf.ones_like(sentence2)\n",
    "    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
    "\n",
    "    inputs = {\n",
    "        'input_word_ids': input_word_ids.to_tensor(),\n",
    "        'input_mask': input_mask,\n",
    "        'input_type_ids': input_type_ids}\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\n",
    "def sentence_token_tagging(test_sentence_tags, tokenized_sentences):\n",
    "    '''\n",
    "    Rewrite the genes found by the NER model. \n",
    "    Input: \n",
    "        1. List of categorical labels asigned by the model during prediction. \n",
    "        2. List of tokenized sentences. (BERT tokenizer)\n",
    "        \n",
    "    Output:\n",
    "        1. NerSentence: Sentences in which each found entity was replaced by the word GENE\n",
    "            [The GENE protein has two activation domains , one of which is an GENE ...]\n",
    "            \n",
    "        2. FinalEntities: All of the entities in the sentence that were replaced by the word GENE\n",
    "            [AraC, arac xyls family domain...]\n",
    "        \n",
    "    '''\n",
    "    entity = ''\n",
    "    num_entities = 0\n",
    "    n = 0\n",
    "    TF_Regulator, RegulatedGene = [],[]\n",
    "    FinalEntities, temp,temp_s,NerSentence = [],[],[],[]\n",
    "    for num in (range(len(test_sentence_tags))): \n",
    "        for num_word, (entity_tags, words) in enumerate(zip(test_sentence_tags[num], tokenized_sentences[num])):\n",
    "            if entity_tags.startswith('B'):\n",
    "                entity += '[SEP] ' + str(words) + ' '\n",
    "                num_entities += 1\n",
    "                temp_s.append('GENE')\n",
    "                \n",
    "            if entity_tags.startswith('I'):\n",
    "                if test_sentence_tags[num][num_word-1].startswith('O'):\n",
    "                    entity += '[SEP] ' + str(words) + ' '\n",
    "                    num_entities += 1\n",
    "                    temp_s.append('GENE')\n",
    "                       \n",
    "                else:\n",
    "                    entity += str(words) + ' '\n",
    "            \n",
    "            if entity_tags.startswith('B') == False and entity_tags.startswith('I') == False:\n",
    "                temp_s.append(words)\n",
    "    \n",
    "        if entity != '':\n",
    "            temp.append(entity.split('[SEP] ')[1:])        \n",
    "        FinalEntities.append(temp)\n",
    "        entity = ''\n",
    "        temp_str = \" \".join(temp_s).replace('  ',' ')\n",
    "        NerSentence.append(temp_str.split(' '))\n",
    "        temp_s = []\n",
    "        temp = []\n",
    "\n",
    "\n",
    "        \n",
    "    print(f'Completed. Found {num_entities} genes.')\n",
    "    return [NerSentence,FinalEntities]\n",
    "\n",
    "\n",
    "def consecutive(data, stepsize=1):\n",
    "    return np.split(data, np.where(np.diff(data) != stepsize)[0]+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 201s 6s/step\n",
      "Completed. Found 4533 genes.\n"
     ]
    }
   ],
   "source": [
    "# Loads the file with the example data and tensorflowhub preprocessor\n",
    "tokenizer = bert.tokenization.FullTokenizer('vocabNER.txt', do_lower_case=False)\n",
    "preprocessor = hub.load( \"https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\")\n",
    "with open('Tools/ris-sentences-ECO.txt','r') as file:\n",
    "    eco_sentence = file.readlines()\n",
    "\n",
    "\n",
    "raw_text = preprocessor(eco_sentence) # Preprocessing sentences\n",
    "bert_classifier = tf.keras.models.load_model('NERModel')  # Loads the NER model trained with the Ner_Training.ipynb notebook\n",
    "prediction = bert_classifier.predict(raw_text) # Predicts the label for each token of the preprocessed sentences\n",
    "\n",
    "# Tokenizes the sentences and removes tokens that are not a whole word \n",
    "pre_txt = []\n",
    "for indx in range(len(eco_sentence)):\n",
    "    pre_txt.append(' '.join(tokenizer.tokenize(eco_sentence[indx])).replace(' ##','').replace('##','').replace('  ',' ').split(' '))\n",
    "    \n",
    "\n",
    "# The \"prediction\" variable has a score for each of the possible categories for a token'\n",
    "#    ([CLS],[SEP],[PAD])        O          B-GENE      I-GENE    -> Index (0:3)\n",
    "#           0.01              0.90         0.05      0.04        -> Predicted score for a single token\n",
    "# THE SIZE OF PREDICTION IS:     (4) x (Num of tokens in a sentence) x (Total num of sentences)\n",
    "# This part finds in which index of \"prediction[sentence x][token x]\" has the bigest number, then saves the asociated label for that index\n",
    "sentence_tags = []\n",
    "raw_sentences = []\n",
    "TF_Regulator_, RegulatedGene_ = [],[]\n",
    "for i, sentence in enumerate(prediction):\n",
    "    temp_rel = []\n",
    "    for n_wor, pred_word in enumerate(sentence):\n",
    "        val = list(pred_word)\n",
    "        if val[1] == max(val):\n",
    "            temp_rel.append('O')\n",
    "        elif val[2] == max(val):\n",
    "            temp_rel.append('B-GENE')\n",
    "        elif val[3] == max(val):\n",
    "            temp_rel.append('I-GENE')\n",
    "    if len(pre_txt[i]) == len(temp_rel):\n",
    "        raw_sentences.append(pre_txt[i])\n",
    "        sentence_tags.append(temp_rel)\n",
    "\n",
    "\n",
    "# Calls the function that hides the found entities.\n",
    "ner_sentence, final_entities  = sentence_token_tagging(sentence_tags,raw_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This part selects all posible pair of entities in a sentence. The 2 selected entities are between a pair of new special tokens [E1]-[/E1] or [E2]-[/E2]\n",
    "# This was aproach is explained here https://medium.com/e-bot7-tech/matching-the-blanks-78e8063794b5 but its not implemented in this code cus it was 2 difficult\n",
    "output_sentence = []\n",
    "entities_to_print = []\n",
    "for line in range(len(ner_sentence)):\n",
    "    new_line = ' '.join(ner_sentence[line])\n",
    "    genes_in_sentence = 0\n",
    "    prducts_in_sentence = 0\n",
    "    for x in range(len(final_entities[line][0])):\n",
    "        if genes_in_sentence == 0:\n",
    "            new_line = new_line.replace('GENE',f'[ {final_entities[line][0][x]} ]',1).replace('  ',' ').replace(' [ ','[E1]').replace(' ] ','[/E1]')\n",
    "            genes_in_sentence += 1            \n",
    "        elif prducts_in_sentence == 0:\n",
    "            new_line = new_line.replace('GENE',f'[ {final_entities[line][0][x]} ]',1).replace('  ',' ').replace(' [ ','[E2]').replace(' ] ','[/E2]')\n",
    "            prducts_in_sentence += 1\n",
    "                         \n",
    "        else:\n",
    "            new_line = new_line.replace('GENE',f' [{final_entities[line][0][x]}] ',1).replace('  ',' ')\n",
    "                \n",
    "    if '[/E2]' in new_line and '[/E1]' in new_line:\n",
    "        output_sentence.append(new_line.replace('  ',' '))\n",
    "\n",
    "\n",
    "\n",
    "# By this part we have sentences in wich two pairs of entities are delimited by [E] and [/E]\n",
    "# Example:\n",
    "##  [E1]AraC[/E1] seems to slightly repress [E2]arac[/E2] ( i . e . , below our cut - off level of 2 . 5 - fold ) .\n",
    "input_sentences = []\n",
    "for e,x in enumerate(output_sentence):\n",
    "    if x[0] == ' ':\n",
    "        output_sentence[e] = output_sentence[e][1:]\n",
    "    temp = ''\n",
    "    list_rel = []\n",
    "    n = 0\n",
    "    for i, word in enumerate(x.split(' ')):\n",
    "        temp += word + ' '\n",
    "        if n == 2:\n",
    "            list_rel[-1] += word + ' '\n",
    "            \n",
    "        if '[/E1]' in word : # All the words located before the end of the first entity ([/E1]) make sentence 1:     i.e [AraC]\n",
    "            list_rel.append(temp)\n",
    "            if n < 1:\n",
    "                temp = ''\n",
    "            n += 1\n",
    "            \n",
    "        if '[/E2]' in word: # All the words located after the end of the first entity ([/E1]) make sentence 2:      i.e [seems to slightly repress arac ( i . e . , below our cut - off level of 2 . 5 - fold ) . ]\n",
    "            list_rel.append(temp)\n",
    "            if n < 1:\n",
    "                temp = ''\n",
    "            n += 1\n",
    "    # The result is the concatenation of sentence 1 and sentence 2 but with a [SEP] token between em.\n",
    "    # [[AraC [SEP] seems to slightly repress arac ( i . e . , below our cut - off level of 2 . 5 - fold ) . ]]\n",
    "    input_sentences.append('[SEP] '.join(list_rel).replace('[/E1]','] ').replace('[/E2]','] ').replace('[E1]',' [').replace('[E2]',' [').replace('  ',' ').replace('[','').replace(']','').replace('  ',' ').split('SEP '))\n",
    "    entities_to_print.append(' '.join(list_rel).replace('[/E1]','] ').replace('[/E2]','] ').replace('[E1]',' [').replace('[E2]',' [').replace('  ',' ').replace('[ ','[').replace(' ]',']').replace('  ',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NarL ] **and NarP proteins compete for the 44 . 5 binding site ; binding of NarP further induces aeg - 46 . 5 operon expression , whereas binding of NarL has no significant effect on the basal anaerobic** ( [ Fnr ] - dependent ) level of expression . \n",
      "\n",
      "[Dna] **A protein appeared to be proportional to the inhibition of RNA polymerase binding to the promoters and the inhibition of transcription from the** promoters . \n",
      "\n",
      "[Nag] **C activates the fimB** promoter ] , but two general scenarios seem most plausible . \n",
      "\n",
      "[A] **rgP directly regulates lysP transcription , we tested binding of ArgP to the lys** P promoter ] / control [ region ] . \n",
      "\n",
      "[DnaA protein] **] and RNA polymerase can coexist at the dnaA promoter by gel - shift and footprinting analyses and whether a direct protein - protein interaction between DnaA protein and RNA polymerase** is the mechanism of inhibition of transcription . \n",
      "\n",
      "[yeaR ] **ogt promoters are also repressed by Fis , we suggest that rapidly growing cells may opt out of certain stress responses , and we** speculate that [ RNS ] may be a small risk in these conditions compared with other stresses . \n",
      "\n",
      "[R] **haR - activated single - copy translational fusions , ( rhaS - lacZ ) 216 and ( rhaS -** lacZ ] ) 92 . \n",
      "\n",
      "[Fi] **s protein is a major factor responsible for catabolite repression at the nrf promoter , and Fis can override activation by FNR and [** NarL ] or [ NarP ] . \n",
      "\n",
      "[Nar] **P protein can activate aeg - 46 . 5 operon expression ( again , presumably by interacting with RNA polymerase ) , the competition from NarL for the binding sites would lead to an antagon** ization of [ NarP ] activation . \n",
      "\n",
      "[hc] **p gene has been shown to be regulated by FNR , [** NarL ] and [ NarP proteins ] . \n",
      "\n",
      "[Dna] **A protein , at concentrations of 75 ng and greater , reduced transcription from rpmH promoter 1P , repression was preferential for the dna** A promoters ] . \n",
      "\n",
      "[] **CpxR , activate nanC expression , although a direct control** has not been proven . \n",
      "\n",
      "[] **OE3 and OE4 , which have higher affinity for GalS than GalR , all have A / T at positions 8 of the operators ( Fig . 1B ) which is also the case for the mglB and galS operators ( Weickert and Adhya , 1993a ,** b ) which are preferentially regulated by [ GalS ] . \n",
      "\n",
      "[] **Rob enhances expression of mdlA , which encodes a multiple - drug - resistance - like ATP - binding component of a transport system ( 3 ) , and strongly enhances expression of micF , the antisense RNA** that downregulates the [ outer ] membrane [ porin OmpF ] . \n",
      "\n",
      "[OmpR ] **micF transcription while binding at far upstream sequences , i . e . in the - 156 to** - 216 region ( Figure 4 ( a ) ) . \n",
      "\n",
      "[MarA ] **upregulates expression of nfnB Both oxygen - insensitive nitroreductases nfsA and nfnB were shown to be upregulated by constitutive expression of MarA from plasmid pAS10 (** Barbosa and Levy , 2000 ) . \n",
      "\n",
      "[His 6 - Arg] **P protein , whereas crude extracts devoid of ArgP gave no retarded band ( Fig . 3B ; see also Fig . 5A ) , indicating that no protein other than ArgP is able to bind significantly to the [** dapB promoter region ] . \n",
      "\n",
      "[Ici] **A protein stimulation of transcription from dnaA promoter 1P in the presence of inhibitory amounts of DnaA** protein ] was examined . \n",
      "\n",
      "[able 1 shows that two amino acid residues of 70 R4 are required for] **Rob - dependent activation of fumC and that only one is** required for activation of [ micF ] . \n",
      "\n",
      "[S1 nuc] **lease - protection assays ( Fig . 4 ) and in vitro run - off transcription assays ( Figs 5 and 6 ) indicate that IciA protein specifically activates transcription from dnaA** promoter 1P ] . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_tags = []\n",
    "raw_example = []\n",
    "all_tags = []\n",
    "for i, sentence in enumerate(prediction):\n",
    "    temp_rel = []\n",
    "    temp_tag = []\n",
    "    for n_wor, pred_word in enumerate(sentence[:len(pre_txt[i])]):\n",
    "        val = list(pred_word)\n",
    "        if val[1] == max(val):\n",
    "            temp_rel.append('O')\n",
    "        elif val[2] == max(val):\n",
    "            temp_rel.append('I-Rel')\n",
    "            temp_tag.append(n_wor)\n",
    "        else:\n",
    "            temp_rel.append('PAD')\n",
    "    if len(pre_txt[i]) == len(temp_rel):\n",
    "        raw_example.append(pre_txt[i])\n",
    "        example_tags.append(temp_rel)\n",
    "        all_tags.append(temp_tag)\n",
    "\n",
    "\n",
    "# for i,x in enumerate(raw_example[0]):\n",
    "#         print(x.replace(' ##','').replace('##',''), example_tags[0][i])\n",
    "\n",
    "predicted_relations = []\n",
    "for sent_num, word_index in enumerate(all_tags):\n",
    "    if len(word_index) > 1:\n",
    "        relationship_end = max(word_index) -1\n",
    "        relationship_start = min(word_index) -2\n",
    "        sentence_relation = ' '.join(raw_example[sent_num][relationship_start:relationship_end])\n",
    "        sentence_start = ' '.join(raw_example[sent_num][:relationship_start]).replace(' ##','').replace('##','')\n",
    "        sentence_end = ' '.join(raw_example[sent_num][relationship_end:]).replace(' ##','').replace('##','')\n",
    "        predicted_relations.append(sentence_start + ' **' + sentence_relation.replace('[ ','').replace(' ]','') + '** ' + sentence_end)\n",
    "    #predicted_relations.append(sentence_relation)\n",
    "\n",
    "for x in predicted_relations[180:200]:\n",
    "    print(x,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier = tf.keras.models.load_model('REModel/')\n",
    "\n",
    "tokenizer = bert.tokenization.FullTokenizer(vocab_file=\"vocabNER.txt\",do_lower_case=False)\n",
    "\n",
    "\n",
    "sentence_test = {}\n",
    "gene1_test = []\n",
    "gene2_test = []\n",
    "full_sentence = []\n",
    "\n",
    "for x in input_sentences:    \n",
    "    gene1_test.append(x[0])\n",
    "    gene2_test.append(x[1])\n",
    "    full_sentence.append(x[0] + x[1])\n",
    "sentence_test['gene1'] = gene1_test\n",
    "sentence_test['gene2'] = gene2_test\n",
    "\n",
    "\n",
    "pre_txt = []\n",
    "for indx in range(len(full_sentence)):\n",
    "    pre_txt.append(' '.join(tokenizer.tokenize(full_sentence[indx])).split(' '))\n",
    "    \n",
    "test_text = bert_encode(sentence_test, tokenizer)\n",
    "prediction = bert_classifier.predict(test_text)\n",
    "\n",
    "visualize_relations = []\n",
    "raw_example = []\n",
    "all_tags = []\n",
    "for i, sentence in enumerate(prediction):\n",
    "    temp_rel = []\n",
    "    temp_tag = []\n",
    "    for n_wor, pred_word in enumerate(sentence[:len(pre_txt[i])]):\n",
    "        val = list(pred_word)\n",
    "        if val[1] == max(val):\n",
    "            temp_rel.append('O')\n",
    "        elif val[2] == max(val):\n",
    "            temp_rel.append('I-Rel')\n",
    "            temp_tag.append(n_wor)\n",
    "        else:\n",
    "            temp_rel.append('PAD')\n",
    "    if len(pre_txt[i]) == len(temp_rel):\n",
    "        raw_example.append(pre_txt[i])\n",
    "        visualize_relations.append(entities_to_print[i])\n",
    "        all_tags.append(temp_tag)\n",
    "\n",
    "for index in range(len(all_tags)):\n",
    "    a = all_tags[index]\n",
    "    a = consecutive(a)\n",
    "    for x in a:\n",
    "        if len(list(x)) > 1:\n",
    "            all_tags[index] = list(x)\n",
    "            \n",
    "\n",
    "relation_pairs = []\n",
    "predicted_relations = []\n",
    "for sent_num, word_index in enumerate(all_tags):\n",
    "    if len(word_index) > 1:\n",
    "        relationship_end = max(word_index)\n",
    "        relationship_start = min(word_index)\n",
    "            \n",
    "        sentence_relation = ' '.join(raw_example[sent_num][relationship_start:relationship_end]).replace(' ##','').replace('##','')\n",
    "        sentence_start = ' '.join(raw_example[sent_num][:relationship_start]).replace(' ##','').replace('##','')\n",
    "        sentence_end = ' '.join(raw_example[sent_num][relationship_end:]).replace(' ##','').replace('##','')\n",
    "        predicted_relations.append(sentence_start + ' **' + sentence_relation + '** ' + sentence_end)\n",
    "        relation_pairs.append(visualize_relations[sent_num])\n",
    "\n",
    "\n",
    "for indx in range(len(predicted_relations)):\n",
    "    relation_start_limit = len(predicted_relations[indx].split('**')[0].split(' '))\n",
    "    while relation_pairs[indx].split(' ')[relation_start_limit][-1] != ']':\n",
    "        relation_start_limit -= 1\n",
    "\n",
    "    relation_end_limit = len(relation_pairs[indx].split(' ')) - len(predicted_relations[indx].split('**')[2].split(' '))\n",
    "    while relation_pairs[indx].split(' ')[relation_end_limit][0] != '[':\n",
    "        relation_end_limit -= 1\n",
    "    \n",
    "    first_unrelared_entities = len(predicted_relations[indx].split('**')[0].split(' '))\n",
    "    while relation_pairs[indx].split(' ')[first_unrelared_entities][0] != '[':\n",
    "        first_unrelared_entities -= 1\n",
    "\n",
    "    second_unrelated_entities = len(relation_pairs[indx].split(' ')) - len(predicted_relations[indx].split('**')[2].split(' '))\n",
    "    while relation_pairs[indx].split(' ')[second_unrelated_entities][-1] != ']':\n",
    "        second_unrelated_entities -= 1\n",
    "\n",
    "    relation = ' '.join(relation_pairs[indx].split(' ')[relation_start_limit + 1:relation_end_limit])\n",
    "    pre_entities_sentence = ' '.join(relation_pairs[indx].split(' ')[:first_unrelared_entities]).replace('[','').replace(']','')\n",
    "    first_entity = '_'.join(relation_pairs[indx].split(' ')[first_unrelared_entities:relation_start_limit + 1])\n",
    "    \n",
    "    pos_entities_sentence = ' '.join(relation_pairs[indx].split(' ')[second_unrelated_entities + 1:]).replace('[','').replace(']','')\n",
    "    second_entity = '_'.join(relation_pairs[indx].split(' ')[relation_end_limit:second_unrelated_entities + 1])\n",
    "    print(pre_entities_sentence + ' ' +  first_entity + ' **' + relation.replace('[','').replace(']','') + '** ' + second_entity + ' ' + pos_entities_sentence,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the present study extend these observations and show that the [MetR_protein] **also stimulates the in vitro expression of metH and that both the MetE and** [MetH_proteins] synthesized in vitro are enzymatically active .  \n",
    "\n",
    "Operons observed to be differentially expressed include not only all four of the transcripts , hmpA , ytfE , [ygbA] **, and hcp - hcr , known or predicted to be** [NsrR] - regulated ( 4 , 40 ) , but also other transcripts predicted to be regulated by nitrite or RNS generated from nitrite , for example , nitric oxide ( 11 ) .  \n",
    "\n",
    "Identification of the DNAbinding domain of the [OmpR_protein] **required for transcriptional activation of the ompF and** [ompC_genes] of Escherichia coli by in vivo DNA footprinting .  \n",
    "\n",
    " [MelR] **is a member of the AraC - XylS family of bacterial gene regulatory proteins ( 6 ) and our previous studies have shown that MelR , together with the cyclic AMP receptor protein , CRP , regulates expression of the** [melAB_operon] that encodes products essential for melibiose metabolism ( 7 ) .  \n",
    "\n",
    "For example , the nitrate reductase genes [narGHJI] **are regulated exclusively by NarL and are not responsive to NarP as observed here for cydD , while nitrate induction of the fdnGHI operon is regulated predominantly by** [NarL] but is also responsive in part to NarP ( 33 , 39 ) .  \n",
    "\n",
    "Under identical experimental conditions as for [argO] **, the regulatory regions of the other ArgP - regulated genes ( asd , dapB , dapD , gdhA , lysA , lysC , and lysP ) were also bound by ArgP , with apparent K d s ranging from 55 nM to 170 nM ; in all these cases ( unlike the situation with argO ) , the addition of** [Lys] was associated with an increase in the apparent K d , indicating that ArgP binding in these instances is Lys sensitive .  \n",
    "\n",
    " [MelR] **- dependent repression of the** [melR_promoter] in each of the new constructs was measured as above and the results are illustrated in Figure 6B .  \n",
    "\n",
    "Given that the activation by full - length , chromosomally expressed [RhaS] **at rhaBAD is approximately 33 - fold higher than that of chromosomally expressed RhaR at rhaSR , comparable efficiencies of activation by the CTDs to their full - length counterparts would have resulted in His 6 - RhaR -** [CTD] activating rhaSR by approximately 30 - fold .  \n",
    "\n",
    "This observation not only allows us to understand how a modest mutation in O NC2 can affect [fimB] **expression whereas the D3 mutation does not , but it also supports our prior assertion that NanR activates fimB expression without** [NagC] ( Sohanpal et al . , 2004 ) .  \n",
    "\n",
    "These latter results , together with the experiments described here showing that [MetJ] **binds to this region of DNA , strongly suggest that the - 8 to + 27 region is involved in** [metE] repression .  \n",
    "\n",
    "Note that in a control experiment , the activity of the MBP - [NarL] **protein was confirmed by its ability to activate transcription of the NarL - dependent** [fdnG_promoter] in vitro ( data not shown ) .  \n",
    "\n",
    "Thus , it appears that NarL and [NarP] **adopt overlapping mechanisms to inhibit**  adopt overlapping mechanisms to inhibit ydhY – T expression .  \n",
    "\n",
    "In relation to pacsP1 , the DNA site for [Fis] **is centred at position – 61 , and its distal nature suggests that** [pacsP1] repression operates via a different mechanism .  \n",
    "\n",
    "Our results indicate that yggA encodes an [ArgP] **- regulated** [Arg_exporter] in E . coli .  \n",
    "\n",
    "Organization and regulation of the [D_-_xylose_operons] **in Escherichia coli K - 12 :** [XylR] acts as a transcriptional activator .  \n",
    "\n",
    "Some of these operons are regulated by the [NarL_protein] **alone , such as the narG and frdA operons , whereas expression of the nirB ( encoding NADH - dependent nitrite reductase ) , nrfA and fdnG operons is controlled by both the** [NarL] and NarP proteins .  \n",
    "\n",
    "The [ArgP_protein] **enhances the expression of the argK gene To test the biological activity of the** [ArgP_protein] on the arginine transport system , the amounts of mRNA synthesized by a 502 bp KpnI - EcoRV DNA fragment containing the N terminus and the control region of the argK gene was investigated .  \n",
    "\n",
    "However , the inactivation of DcuS ( [IMW553] **) led to a decrease of citC - lacZ expression by a factor of 1 . 5 , which could be due to interaction between CitA and** [DcuS] .  \n",
    "\n",
    "RESULTS Oligomerization of DnaA Protein to the [dnaA_Promoter] **— Sequence - specific DNA binding of DnaA protein to the** [dnaA_promoter_region] ( Fig . 1 ) was studied in detail using a combined gel - shift and chemical footprinting assay to determine the extent of DNA binding to the region flanking the consensus DNA - binding site , the DnaA box .  \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee7d7838ef53998fd22ad7449b76e48b4013ea11e59d28ee193f2cd757746339"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
