{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as hub_text  # A dependency of the preprocessing model\n",
    "from official import nlp\n",
    "import official.nlp.optimization\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from sklearn.metrics import classification_report\n",
    "import math\n",
    "import official.nlp.bert.tokenization\n",
    "from official.nlp import bert\n",
    "\n",
    "\n",
    "def load_ris_file(file_ds):\n",
    "    '''\n",
    "    Loads the ner dataset. Replace categotical variables for numerical variables:'\n",
    "    O = 1   B-GENE = 2    I-GENE = 3    [CLS],[SEP],[PAD] = 0\n",
    "    Output:\n",
    "    1. text_test: a list of sentences containing a list of words. \n",
    "    2. label_test: a lost of lists containing labels for each word in each sentence.\n",
    "    '''\n",
    "    with open(f'DatasetNER/{file_ds}.tsv','r') as file:\n",
    "        test_file = file.readlines()\n",
    "\n",
    "    text_test = []\n",
    "    label_test = []\n",
    "    temp_ds = []\n",
    "    temp_label = []\n",
    "    for line in test_file:\n",
    "        if line == '\\n':  \n",
    "            if len(temp_label) > 128:\n",
    "                temp_label = temp_label[:128]\n",
    "            text_test.append(' '.join(temp_ds))\n",
    "            if len(temp_label) < 128:\n",
    "                temp_label.extend([0]*(128-len(temp_label)))\n",
    "            label_test.append(temp_label)\n",
    "            temp_ds = []\n",
    "            temp_label = []\n",
    "        else:\n",
    "            temp_ds.append(line.split('\\t')[0].replace('\\n',''))\n",
    "            if line.split('\\t')[1].replace('\\n','') == 'O':\n",
    "                temp_label.append(1)\n",
    "            elif line.split('\\t')[1].replace('\\n','') == 'B-GENE':\n",
    "                temp_label.append(2)\n",
    "            elif line.split('\\t')[1].replace('\\n','') == 'I-GENE':\n",
    "                temp_label.append(3)\n",
    "    \n",
    "    return text_test,label_test\n",
    "\n",
    "\n",
    "\n",
    "def build_classifier_model(num_classes):\n",
    "    '''\n",
    "    Fine tunes BERT.\n",
    "    Input: Dataset made by a bert preprocessor e.i \"https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\"\n",
    "        1. input_word_ids, input_mask, input_type_ids\n",
    "        2. Labels for each word. \n",
    "    Output: Trained model.\n",
    "    '''\n",
    "\n",
    "    inputs = dict(\n",
    "        input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
    "        input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
    "        input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
    "  )\n",
    "  \n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='encoder')\n",
    "    net = encoder(inputs)['sequence_output']\n",
    "    \n",
    "    net = tf.keras.layers.Dropout(rate=0.1)(net)\n",
    "    net = tf.keras.layers.Dense(num_classes, activation='softmax', name='classifier')(net)\n",
    "    return tf.keras.Model(inputs, net, name='prediction')\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    '''\n",
    "    Controls what is happening during training.\n",
    "    '''\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = bert_classifier(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss_value, bert_classifier.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, bert_classifier.trainable_weights))\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    '''\n",
    "    Computes the validation accuracy for each epoch during training.\n",
    "    '''\n",
    "    val_logits = bert_classifier(x, training=False)\n",
    "    val_acc_metric.update_state(y, val_logits)\n",
    "\n",
    "    \n",
    "    \n",
    "def sentence_token_tagging(test_sentence_tags, tokenized_sentences):\n",
    "    '''\n",
    "    Rewrite the found genes during training. \n",
    "    Input: \n",
    "        1. List of categorical labels asigned by the model during prediction. \n",
    "        2. List of tokenized sentences. (BERT tokenizer)\n",
    "        \n",
    "    Output:\n",
    "        1. NerSentence: Sentences in which each found entity was replaced by the word GENE\n",
    "            [The GENE protein has two activation domains , one of which is an GENE ...]\n",
    "            \n",
    "        2. FinalEntities: All of the entities in the sentence that were replaced by the word GENE\n",
    "            [AraC, arac xyls family domain...]\n",
    "        \n",
    "    '''\n",
    "    entity = ''\n",
    "    num_entities = 0\n",
    "    n = 0\n",
    "    FinalEntities, temp,temp_s,NerSentence = [],[],[],[]\n",
    "    for num in (range(len(test_sentence_tags))): \n",
    "        for num_word, (entity_tags, words) in enumerate(zip(test_sentence_tags[num], tokenized_sentences[num])):\n",
    "            if entity_tags.startswith('B'):\n",
    "                n += 1\n",
    "                if n == 1:\n",
    "                    entity += '[SEP]' + str(words) + ' '\n",
    "                else:\n",
    "                    entity += str(words) + ' '\n",
    "                temp_s.append('GENE')\n",
    "                num_entities += 1\n",
    "            elif entity_tags.startswith('I'):\n",
    "                entity += str(words) + ' '\n",
    "                temp_s[-1] += ' GENE'\n",
    "                n = 0\n",
    "            else:\n",
    "                temp_s.append(words)\n",
    "                n = 0\n",
    "            if entity != '':\n",
    "                temp.append(entity.replace(' ##','').replace('##',''))\n",
    "            else:\n",
    "                temp.append('')\n",
    "        FinalEntities.append(temp[-1])\n",
    "        entity = ''\n",
    "        temp_str = \" \".join(temp_s).replace(' ##','').replace('##','').replace('  ',' ')\n",
    "        NerSentence.append(temp_str)\n",
    "        temp_s = []\n",
    "        temp = []\n",
    "        \n",
    "    print(f'Completed. Found {num_entities} genes.')\n",
    "    return [NerSentence,FinalEntities]\n",
    "\n",
    "\n",
    "# Loads the preprocessor and the BERT model to fine tune from TensorflowHub  \n",
    "preprocessor = hub.load( \"https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\")\n",
    "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_train, label_train = load_ris_file('train')\n",
    "train_x = preprocessor(text_train)\n",
    "tokens = preprocessor.tokenize(text_train)\n",
    "\n",
    "# Since BERT tokenizer splits single words (interestingly -> interest, ##ing, ##ly), we need to add labels for each additional token\n",
    "              #data                 #labels\n",
    "# Before:   interest ##ing #ly      -> O\n",
    "# After:    interest ##ing #ly      -> O, 0, 0\n",
    "for num_s, sentence in enumerate(tokens):\n",
    "    n = 0\n",
    "    for num_w, word in enumerate(sentence):\n",
    "        if num_w < 128:\n",
    "            old_tag = label_train[num_s][num_w+n]\n",
    "        if len(word) > 1:\n",
    "            for subtoken in range(len(word)-1):\n",
    "                if old_tag == 1:\n",
    "                    label_train[num_s].insert(num_w + n ,0)\n",
    "                elif old_tag == 2:\n",
    "                    label_train[num_s].insert(num_w + n ,0)\n",
    "                elif old_tag == 3:\n",
    "                    label_train[num_s].insert(num_w + n ,0)\n",
    "            n += len(word) -1\n",
    "    label_train[num_s].insert(0,0) # Adds a 0 for the [CLS] tokken added by the BERT preprocessor. \n",
    "    label_train[num_s] = tf.constant(label_train[num_s][0:128]) # Controls max length\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x,label_train)) # Builds dataset in a format that can be fed to the model.\n",
    "\n",
    "\n",
    "# Optimizador, metricas y loss function. \n",
    "# Set up epochs and steps\n",
    "epochs = 2\n",
    "batch_size = 32\n",
    "train_data_size = len(label_train)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer = nlp.optimization.create_optimizer(3e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n",
    "bert_classifier = build_classifier_model(4)\n",
    "train_ds = train_dataset.shuffle(len(label_train),reshuffle_each_iteration=True)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "# Metrics and loss that can be used by a multiclass classificator\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but fot the test dataset.\n",
    "text_test,label_test = load_ris_file('test')\n",
    "test_x = preprocessor(text_test)\n",
    "tokens = preprocessor.tokenize(text_test)\n",
    "for num_s, sentence in enumerate(tokens):\n",
    "    n = 0\n",
    "    for num_w, word in enumerate(sentence):\n",
    "        if num_w < 128:\n",
    "            old_tag = label_test[num_s][num_w+n]\n",
    "        if len(word) > 1:\n",
    "            for subtoken in range(len(word)-1):\n",
    "                if old_tag == 1:\n",
    "                    label_test[num_s].insert(num_w + n ,0)\n",
    "                elif old_tag == 2:\n",
    "                    label_test[num_s].insert(num_w + n ,0)\n",
    "                elif old_tag == 3:\n",
    "                    label_test[num_s].insert(num_w + n ,0)\n",
    "            n += len(word) -1\n",
    "    label_test[num_s].insert(0,0)\n",
    "    label_test[num_s] = tf.constant(label_test[num_s][0:128])\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((test_x,label_test))\n",
    "val_ds = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.1518588066101074\n",
      "Training loss (for one batch) at step 50: 0.12621662020683289\n",
      "Training loss (for one batch) at step 100: 0.03737030178308487\n",
      "Training loss (for one batch) at step 150: 0.04553067311644554\n",
      "Training loss (for one batch) at step 200: 0.028643585741519928\n",
      "Training loss (for one batch) at step 250: 0.026299865916371346\n",
      "Training loss (for one batch) at step 300: 0.023588906973600388\n",
      "Training loss (for one batch) at step 350: 0.03769940137863159\n",
      "Training loss (for one batch) at step 400: 0.018895652145147324\n",
      "Training loss (for one batch) at step 450: 0.019236121326684952\n",
      "Training accuracy over epoch 0: 0.945479154586792\n",
      "Validation acc: 0.9918921589851379\n",
      "Time taken: 12532.133661985397\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.023592986166477203\n",
      "Training loss (for one batch) at step 50: 0.02015260048210621\n",
      "Training loss (for one batch) at step 100: 0.012883375398814678\n",
      "Training loss (for one batch) at step 150: 0.013862237334251404\n",
      "Training loss (for one batch) at step 200: 0.021557003259658813\n",
      "Training loss (for one batch) at step 250: 0.020952148362994194\n",
      "Training loss (for one batch) at step 300: 0.017164137214422226\n",
      "Training loss (for one batch) at step 350: 0.025161949917674065\n",
      "Training loss (for one batch) at step 400: 0.03054489940404892\n",
      "Training loss (for one batch) at step 450: 0.019955337047576904\n",
      "Training accuracy over epoch 1: 0.9924328327178955\n",
      "Validation acc: 0.9931296706199646\n",
      "Time taken: 12374.974542856216\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Custom training loop. https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\n",
    "epoch_bar = master_bar(range(epochs))\n",
    "pb_max_len = math.ceil(float(len(text_train))/float(batch_size))\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "    for step, (x_batch_train, y_batch_train) in progress_bar(enumerate(train_ds),total=pb_max_len, parent=epoch_bar):\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print(f\"Training loss (for one batch) at step {step}: {loss_value}\")\n",
    "\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(f\"Training accuracy over epoch {epoch}: {float(train_acc)}\")\n",
    "    train_acc_metric.reset_states()\n",
    "    \n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_ds:\n",
    "        test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(f\"Validation acc: {float(val_acc)}\")\n",
    "    print(f\"Time taken: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"prediction\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " encoder (KerasLayer)           {'encoder_outputs':  108310273   ['input_2[0][0]',                \n",
      "                                 [(None, None, 768)               'input_3[0][0]',                \n",
      "                                , (None, None, 768)               'input_1[0][0]']                \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                ],                                                                \n",
      "                                 'default': (None,                                                \n",
      "                                768),                                                             \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, None, 768),                                               \n",
      "                                 'pooled_output': (                                               \n",
      "                                None, 768)}                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 768)    0           ['encoder[0][14]']               \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, None, 4)      3076        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,313,349\n",
      "Trainable params: 108,313,348\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 366). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: NERModel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: NERModel/assets\n"
     ]
    }
   ],
   "source": [
    "# Prints a model summary and saves the model.\n",
    "bert_classifier.summary()\n",
    "bert_classifier.save('NERModel/', include_optimizer=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee7d7838ef53998fd22ad7449b76e48b4013ea11e59d28ee193f2cd757746339"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
