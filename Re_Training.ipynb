{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as tf_text  # A dependency of the preprocessing model\n",
    "from official import nlp\n",
    "import official.nlp.optimization\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import math\n",
    "import official.nlp.bert.tokenization\n",
    "from official.nlp import bert\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_classifier_model(num_classes):\n",
    "    '''\n",
    "    Fine tunes BERT.\n",
    "    Input: Dataset made by a bert preprocessor e.i \"https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\"\n",
    "        1. input_word_ids, input_mask, input_type_ids\n",
    "        2. Labels for each word. \n",
    "    Output: Trained model.\n",
    "    '''\n",
    "\n",
    "    inputs = dict(\n",
    "        input_word_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
    "        input_mask=tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
    "        input_type_ids=tf.keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
    "    )\n",
    "  \n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='encoder')\n",
    "    net = encoder(inputs)['sequence_output']\n",
    "    net = tf.keras.layers.Dropout(rate=0.1)(net)\n",
    "    net = tf.keras.layers.Dense(num_classes, activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(inputs, net, name='prediction')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    '''\n",
    "    Controls what is happening during training.\n",
    "    '''\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = bert_classifier(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss_value, bert_classifier.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, bert_classifier.trainable_weights))\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    '''\n",
    "    Computes the validation accuracy for each epoch during training.\n",
    "    '''\n",
    "    val_logits = bert_classifier(x, training=True)\n",
    "    val_acc_metric.update_state(y, val_logits)\n",
    "    \n",
    "\n",
    "def remove_subtoken(text_var,label_var):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    tokens = preprocessor.tokenize(text_var)\n",
    "    for num_s, sentence in enumerate(tokens):\n",
    "        n = 0\n",
    "        for num_w, word in enumerate(sentence):\n",
    "            if num_w < 128:\n",
    "                old_tag = label_var[num_s][num_w+n]\n",
    "            if len(word) > 1:\n",
    "                for subtoken in range(len(word)-1):\n",
    "                    if old_tag == 1:\n",
    "                        label_var[num_s].insert(num_w + n ,0)\n",
    "                    elif old_tag == 2:\n",
    "                        label_var[num_s].insert(num_w + n ,0)\n",
    "                    if n < len(label_var[num_s]):\n",
    "                        n += 1\n",
    "        label_var[num_s].insert(0,0)\n",
    "        label_var[num_s] = tf.constant(label_var[num_s][0:128])\n",
    "\n",
    "    \n",
    "    \n",
    "def encode_sentence(s, tokenizer):\n",
    "    '''\n",
    "    Tokenizes pair of sentences and adds a [SEP] token to join them. This token is labeled as 0\n",
    "    '''\n",
    "    tokens = list(tokenizer.tokenize(s))\n",
    "    tokens.append('[SEP]')\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(sentence_dict, tokenizer):\n",
    "    '''\n",
    "    Preprocess the data to be on the format expected by BERT. Does the same\n",
    "    as the BERT preprocessor function.\n",
    "    Input:\n",
    "        1. Dict containing:\n",
    "            'sentence1':\n",
    "                ['These results indicate that the GeneReg'], \n",
    "            'sentence2':\n",
    "                ['and acrD drug efflux genes are directly regulated by RegProtein protein ( BaeR protein ) .']\n",
    "        2. Labels:\n",
    "            [''O O O O O O 0 I-Rel I-Rel I-Rel I-Rel I-Rel I-Rel I-Rel I-Rel I-Rel O O O O O O O O]\n",
    "    '''\n",
    "    num_examples = len(sentence_dict[\"gene1\"])\n",
    "\n",
    "    sentence1 = tf.ragged.constant([\n",
    "        encode_sentence(s, tokenizer)\n",
    "        for s in np.array(sentence_dict[\"gene1\"])])\n",
    "    sentence2 = tf.ragged.constant([\n",
    "        encode_sentence(s, tokenizer)\n",
    "        for s in np.array(sentence_dict[\"gene2\"])])\n",
    "\n",
    "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n",
    "    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
    "\n",
    "    input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "\n",
    "    type_cls = tf.zeros_like(cls)\n",
    "    type_s1 = tf.zeros_like(sentence1)\n",
    "    type_s2 = tf.ones_like(sentence2)\n",
    "    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
    "\n",
    "    inputs = {\n",
    "        'input_word_ids': input_word_ids.to_tensor()[0:, :128],\n",
    "        'input_mask': input_mask[0:, :128],\n",
    "        'input_type_ids': input_type_ids[0:, :128]}\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "    \n",
    "preprocessor = hub.load( \"https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\")\n",
    "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'DatasetRE/train_mnli.txt','r') as file:\n",
    "    test_file = file.readlines()\n",
    "    \n",
    "sentence_train = {}\n",
    "train_labels = []\n",
    "gene1_train = []\n",
    "gene2_train = []\n",
    "full_train_sentence = []\n",
    "\n",
    "for x in test_file:\n",
    "    if x != '\\n':\n",
    "        # Each variable has 1 of the 2 input sentences.\n",
    "        gene1_train.append(x.split('\\t')[0])\n",
    "        gene2_train.append(x.split('\\t')[1])\n",
    "        \n",
    "        # Replaces categotical variables for numerical variables:'\n",
    "        #    O = 1   I-Rel = 2     [CLS],[SEP],[PAD] = 0\n",
    "        temp_label = []\n",
    "        for tag in x.split('\\t')[2].split(' '):\n",
    "            if tag == 'O':\n",
    "                temp_label.append(1)\n",
    "            if tag == 'I-Rel':\n",
    "                temp_label.append(2)\n",
    "        if len(temp_label) > 128:\n",
    "            temp_label = temp_label[:128]\n",
    "        else:\n",
    "            if len(temp_label) < 128:\n",
    "                temp_label.extend([0]*(128-len(temp_label)))\n",
    "        train_labels.append(temp_label)\n",
    "        \n",
    "        # Single list containing the 2 concatenated input sentences  \n",
    "        full_train_sentence.append(x.split('\\t')[0] + x.split('\\t')[1])\n",
    "sentence_train['gene1'] = gene1_train\n",
    "sentence_train['gene2'] = gene2_train\n",
    "\n",
    "\n",
    "# Loads the tokenizer and calls the preprocessing function.\n",
    "tokenizer = bert.tokenization.FullTokenizer(vocab_file=\"vocabNER.txt\",do_lower_case=False)\n",
    "train_ds = bert_encode(sentence_train, tokenizer)\n",
    "\n",
    "\n",
    "# Tokenizes each sentence\n",
    "pre_txt = []\n",
    "for indx in range(len(full_train_sentence)):\n",
    "    pre_txt.append(' '.join(tokenizer.tokenize(full_train_sentence[indx])).split(' '))\n",
    "    \n",
    "    \n",
    "# Since BERT tokenizer splits single words (interestingly -> interest, ##ing, ##ly), we need to add labels for each additional token\n",
    "#                     data                        labels\n",
    "# Input example:      interest ##ing #ly          -> O\n",
    "# Output example:     interest ##ing #ly          -> O, 0, 0\n",
    "for index in range(len(pre_txt)):\n",
    "    for x in range(len(pre_txt[index])):\n",
    "        if '##' in pre_txt[index][x]:\n",
    "            train_labels[index].insert(x,0)\n",
    "    train_labels[index] = tf.constant(train_labels[index][:128])\n",
    "\n",
    "\n",
    "# Builds dataset in a format that can be fed to the model.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_ds,train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but fot the test dataset.\n",
    "with open(f'DatasetRE/test_mnli.txt','r') as file:\n",
    "    test_file = file.readlines()\n",
    "    \n",
    "sentence_test = {}\n",
    "test_label = []\n",
    "gene1_test = []\n",
    "gene2_test = []\n",
    "full_test_sentence = []\n",
    "for x in test_file:\n",
    "    if x != '\\n':\n",
    "        gene1_test.append(x.split('\\t')[0])\n",
    "        gene2_test.append(x.split('\\t')[1])\n",
    "        temp_label = []\n",
    "        for tag in x.split('\\t')[2].split(' '):\n",
    "            if tag == 'O':\n",
    "                temp_label.append(1)\n",
    "            if tag == 'I-Rel':\n",
    "                temp_label.append(2)\n",
    "        if len(temp_label) > 128:\n",
    "            temp_label = temp_label[:128]\n",
    "        else:\n",
    "            if len(temp_label) < 128:\n",
    "                temp_label.extend([0]*(128-len(temp_label)))\n",
    "        test_label.append(temp_label)\n",
    "        full_test_sentence.append(x.split('\\t')[0] + x.split('\\t')[1])\n",
    "sentence_test['gene1'] = gene1_test\n",
    "sentence_test['gene2'] = gene2_test\n",
    "\n",
    "tokenizer = bert.tokenization.FullTokenizer(vocab_file=\"vocabNER.txt\",do_lower_case=False)\n",
    "test_ds = bert_encode(sentence_test, tokenizer)\n",
    "\n",
    "pre_txt = []\n",
    "for indx in range(len(full_test_sentence)):\n",
    "    pre_txt.append(' '.join(tokenizer.tokenize(full_test_sentence[indx])).split(' '))\n",
    "    \n",
    "for index in range(len(pre_txt)):\n",
    "    for x in range(len(pre_txt[index])):\n",
    "        if '##' in pre_txt[index][x]:\n",
    "            test_label[index].insert(x,0)\n",
    "    test_label[index] = tf.constant(test_label[index][:128])\n",
    "    \n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_ds,test_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizador, metricas y loss function. \n",
    "# Set up epochs and steps\n",
    "epochs = 2\n",
    "batch_size = 32\n",
    "train_data_size = len(train_labels)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer = nlp.optimization.create_optimizer(init_lr=2e-5,num_train_steps=num_train_steps,num_warmup_steps=warmup_steps)\n",
    "bert_classifier = build_classifier_model(3)\n",
    "train_ds = train_ds.shuffle(len(train_labels),reshuffle_each_iteration=True)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "# Metrics and loss that can be used by a multiclass classificator\n",
    "val_ds = test_ds.batch(batch_size)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "#bert_classifier.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 1.1509202718734741\n",
      "Training loss (for one batch) at step 25: 0.4241214990615845\n",
      "Training accuracy over epoch 0: 0.7838122844696045\n",
      "Validation acc: 0.883280336856842\n",
      "Time taken: 1538.9019479751587\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 0.22367316484451294\n",
      "Training loss (for one batch) at step 25: 0.1800626963376999\n",
      "Training accuracy over epoch 1: 0.9231259822845459\n",
      "Validation acc: 0.9237743020057678\n",
      "Time taken: 1523.4775159358978\n"
     ]
    }
   ],
   "source": [
    "# Custom training loop. https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\n",
    "epoch_bar = master_bar(range(epochs))\n",
    "pb_max_len = math.ceil(float(len(train_labels))/float(batch_size))\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "    print(\"\\nStart of epoch %d\" % (epoch + 1,))\n",
    "    start_time = time.time()\n",
    "    for step, (x_batch_train, y_batch_train) in progress_bar(enumerate(train_ds),total=pb_max_len, parent=epoch_bar):\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "        \n",
    "        if step % 25 == 0:\n",
    "            print(f\"Training loss (for one batch) at step {step}: {loss_value}\")\n",
    "\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(f\"Training accuracy over epoch {epoch}: {float(train_acc)}\")\n",
    "    train_acc_metric.reset_states()\n",
    "    \n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_ds:\n",
    "        test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(f\"Validation acc: {float(val_acc)}\")\n",
    "    print(f\"Time taken: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"prediction\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " encoder (KerasLayer)           {'default': (None,   108310273   ['input_2[0][0]',                \n",
      "                                768),                             'input_3[0][0]',                \n",
      "                                 'pooled_output': (               'input_1[0][0]']                \n",
      "                                None, 768),                                                       \n",
      "                                 'sequence_output':                                               \n",
      "                                 (None, None, 768),                                               \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                , (None, None, 768)                                               \n",
      "                                ]}                                                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 768)    0           ['encoder[0][14]']               \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, None, 3)      2307        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,312,580\n",
      "Trainable params: 108,312,579\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 366). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reModel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reModel/assets\n"
     ]
    }
   ],
   "source": [
    "# Prints a model summary and saves the model.\n",
    "bert_classifier.summary()\n",
    "# #tf.keras.utils.plot_model(bert_classifier, \"RE_Interactions.png\", show_shapes=True)\n",
    "bert_classifier.save('reModel/', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee7d7838ef53998fd22ad7449b76e48b4013ea11e59d28ee193f2cd757746339"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
