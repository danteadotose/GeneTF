{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as hub_text  # A dependency of the preprocessing model\n",
    "from official import nlp\n",
    "import official.nlp.optimization\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import math\n",
    "import official.nlp.bert.tokenization\n",
    "from official.nlp import bert\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "import re\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "def sentence_token_taging(test_sentence_tags, tokenized_sentences, regulator_ds,regulated_ds):\n",
    "    entity = ''\n",
    "    num_entities = 0\n",
    "    n = 0\n",
    "    TF_Regulator, RegulatedGene = [],[]\n",
    "    FinalEntities, temp,temp_s,NerSentence = [],[],[],[]\n",
    "    for num in (range(len(test_sentence_tags))): \n",
    "        for num_word, (entity_tags, words) in enumerate(zip(test_sentence_tags[num], tokenized_sentences[num])):\n",
    "            if entity_tags.startswith('B'):\n",
    "                entity += '[SEP] ' + str(words) + ' '\n",
    "                num_entities += 1\n",
    "                temp_s.append('GENE')\n",
    "                \n",
    "            if entity_tags.startswith('I'):\n",
    "                if test_sentence_tags[num][num_word-1].startswith('O'):\n",
    "                    entity += '[SEP] ' + str(words) + ' '\n",
    "                    num_entities += 1\n",
    "                    temp_s.append('GENE')\n",
    "                       \n",
    "                else:\n",
    "                    entity += str(words) + ' '\n",
    "            \n",
    "            if entity_tags.startswith('B') == False and entity_tags.startswith('I') == False:\n",
    "                temp_s.append(words)\n",
    "    \n",
    "        if entity != '':\n",
    "            temp.append(entity.split('[SEP] ')[1:])        \n",
    "        FinalEntities.append(temp)\n",
    "        entity = ''\n",
    "        temp_str = \" \".join(temp_s).replace('  ',' ')\n",
    "        NerSentence.append(temp_str.split(' '))\n",
    "        temp_s = []\n",
    "        temp = []\n",
    "        TF_Regulator.append(regulator_ds[num])\n",
    "        RegulatedGene.append(regulated_ds[num])\n",
    "\n",
    "        \n",
    "    print(f'Completed. Found {num_entities} genes.')\n",
    "    return [NerSentence,TF_Regulator,RegulatedGene,FinalEntities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 315s 6s/step\n"
     ]
    }
   ],
   "source": [
    "preprocessor = hub.load( \"https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\")\n",
    "# ris_eco = pd.read_csv('../DatasetNER/STM-ris-curadas.csv').fillna(0)\n",
    "# eco_sentence = list(ris_eco['SENTENCE'])\n",
    "# eco_regulator = list(ris_eco['TF'])\n",
    "# eco_regulated = list(ris_eco['GENE/TU'])\n",
    "\n",
    "ris_eco = pd.read_csv('../DatasetNER/ECO-ris-curadas.csv').fillna(0)\n",
    "eco_sentence = ris_eco['SENTENCE']\n",
    "eco_regulator = list(ris_eco['REGULATOR'])\n",
    "eco_effect =  list(ris_eco['EFFECT'])\n",
    "\n",
    "\n",
    "eco_regulated = []\n",
    "for index in range(len(eco_regulator)):\n",
    "    if list(ris_eco['REGULATED_GENE'])[index] == 0:\n",
    "        eco_regulated.append(list(ris_eco['REGULATED_TU'])[index])\n",
    "    else:\n",
    "        eco_regulated.append(list(ris_eco['REGULATED_GENE'])[index])\n",
    "\n",
    "\n",
    "raw_text = preprocessor(eco_sentence)\n",
    "bert_classifier = tf.keras.models.load_model('../NERModel')\n",
    "prediction = bert_classifier.predict(raw_text)\n",
    "tokenizer = bert.tokenization.FullTokenizer('../vocabNER.txt', do_lower_case=False)\n",
    "\n",
    "pre_txt = []\n",
    "for indx in range(len(eco_sentence)):\n",
    "    pre_txt.append(' '.join(tokenizer.tokenize(eco_sentence[indx])).replace(' ##','').replace('##','').replace('  ',' ').split(' '))\n",
    "    \n",
    "sentence_tags = []\n",
    "raw_sentences = []\n",
    "TF_Regulator_, RegulatedGene_ = [],[]\n",
    "for i, sentence in enumerate(prediction):\n",
    "    temp_rel = []\n",
    "    for n_wor, pred_word in enumerate(sentence):\n",
    "        val = list(pred_word)\n",
    "        if val[1] == max(val):\n",
    "            temp_rel.append('O')\n",
    "        elif val[2] == max(val):\n",
    "            temp_rel.append('B-GENE')\n",
    "        elif val[3] == max(val):\n",
    "            temp_rel.append('I-GENE')\n",
    "    if len(pre_txt[i]) == len(temp_rel):\n",
    "        raw_sentences.append(pre_txt[i])\n",
    "        sentence_tags.append(temp_rel)\n",
    "        RegulatedGene_.append(eco_regulated[i])\n",
    "        TF_Regulator_.append(eco_regulator[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed. Found 7628 genes.\n"
     ]
    }
   ],
   "source": [
    "ner_sentence,tf_regulator, regulated_gene, final_entities  = sentence_token_taging(sentence_tags,raw_sentences, TF_Regulator_, RegulatedGene_)\n",
    "\n",
    "\n",
    "output_sentence = []\n",
    "for line in range(len(ner_sentence)):\n",
    "    new_line = ' '.join(ner_sentence[line])\n",
    "    genes_in_sentence = 0\n",
    "    prducts_in_sentence = 0\n",
    "    if final_entities[line]:\n",
    "        for x in range(len(final_entities[line][0])):\n",
    "            if genes_in_sentence == 0:\n",
    "                en = final_entities[line][0][x].replace(TF_Regulator_[line].split(' ')[0],'RegProtein',1)\n",
    "                if 'RegProtein' in en:\n",
    "                    new_line = new_line.replace('GENE',f'[ {en}]',1).replace('  ',' ').replace('[ ','[E1]').replace(' ]','[/E1]')\n",
    "                    genes_in_sentence += 1\n",
    "            else:\n",
    "                new_line = new_line.replace('GENE',final_entities[line][0][x],1).replace('  ',' ')\n",
    "\n",
    "            if prducts_in_sentence == 0:\n",
    "                te = final_entities[line][0][x].replace(RegulatedGene_[line].split(' ')[0],'GeneReg',1)\n",
    "                if 'GeneReg' in te:\n",
    "                    new_line = new_line.replace('GENE',f'[ {te}]',1).replace('  ',' ').replace('[ ','[E2]').replace(' ]','[/E2]')\n",
    "                    prducts_in_sentence += 1\n",
    "            else:\n",
    "                new_line = new_line.replace('GENE',final_entities[line][0][x],1).replace('  ',' ')\n",
    "                    \n",
    "            if genes_in_sentence + prducts_in_sentence <= 1:\n",
    "                if ('RegProtein' != en.split(' ')[1]) and ('GeneReg' not in te):\n",
    "                    new_line = new_line.replace('GENE',final_entities[line][0][x],1).replace('  ',' ')\n",
    "                    \n",
    "        if '[/E2]' in new_line and '[/E1]' in new_line:\n",
    "            output_sentence.append(new_line.replace('  ',' '))\n",
    "        \n",
    "# input_sentences = list(set(output_sentence))\n",
    "\n",
    "input_sentences = []\n",
    "for e,x in enumerate(output_sentence):\n",
    "    if x[0] == ' ':\n",
    "        output_sentence[e] = output_sentence[e][1:]\n",
    "    temp = ''\n",
    "    list_rel = []\n",
    "    n = 0\n",
    "    for i, word in enumerate(x.split(' ')):\n",
    "        temp += word + ' '\n",
    "        if n == 2:\n",
    "            list_rel[-1] += word + ' '\n",
    "        if '[/E1]' in word :\n",
    "            list_rel.append(temp)\n",
    "            if n < 1:\n",
    "                temp = ''\n",
    "            n += 1\n",
    "        if '[/E2]' in word:\n",
    "            list_rel.append(temp)\n",
    "            if n < 1:\n",
    "                temp = ''\n",
    "            n += 1\n",
    "    element = '[SEP] '.join(list_rel).replace('[/E1]','').replace('[/E2]','').replace('[E1]','').replace('[E2]','').replace('  ','')\n",
    "    input_sentences.append(element)\n",
    "    \n",
    "\n",
    "labels = []\n",
    "sentences = []\n",
    "for indx in range(len(input_sentences)):\n",
    "    temp_label = []\n",
    "    n = 0\n",
    "    for word in input_sentences[indx].replace('  ',' ').split(' '):\n",
    "        if n == 1 and '[SEP]' not in word and word != 'GeneReg' and word != 'RegProtein':\n",
    "            temp_label.append('I-Rel')\n",
    "        if '[SEP]' in word:\n",
    "            n = 1\n",
    "            temp_label.append('0')\n",
    "        if (word == 'GeneReg' or word == 'RegProtein') and n == 1:\n",
    "            n = 2\n",
    "            #temp_label.append('O')\n",
    "        #else:\n",
    "        if n == 0:\n",
    "            temp_label.append('O')\n",
    "        if n == 2:\n",
    "            temp_label.append('O')\n",
    "    sentences.append(input_sentences[indx].replace('  ',' ').split(' [SEP]'))\n",
    "    labels.append(temp_label)\n",
    "    \n",
    "with open('../DatasetRE/test_mnli.txt','w+') as file:\n",
    "    for i, line in enumerate(sentences):\n",
    "        for e,segment in enumerate(line):\n",
    "            file.write(segment)\n",
    "            file.write('\\t')\n",
    "        for tag in labels[i]:\n",
    "            file.write(str(tag))\n",
    "            file.write(' ')\n",
    "        file.write('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee7d7838ef53998fd22ad7449b76e48b4013ea11e59d28ee193f2cd757746339"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
